{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261f648d-d91b-4b9e-90f9-98d80ef9963a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01b038e8-dbf4-400e-91d3-477f03ed71d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7352 rows and 563 columns\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/train.csv', delimiter=',')\n",
    "df_train.dataframeName = 'train.csv'\n",
    "nRow, nCol = df_train.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8780fc5e-ee1f-46e1-ac3d-c127afe588ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2947 rows and 563 columns\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/test.csv', delimiter=',')\n",
    "df_test.dataframeName = 'test.csv'\n",
    "nRow, nCol = df_test.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a9b6d9c-f127-48fa-9c2f-909cd3607d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_x1 = df_train['tBodyAcc-mean()-X']\n",
    "acc_y1 = df_train['tBodyAcc-mean()-Y']\n",
    "acc_z1 = df_train['tBodyAcc-mean()-Z']\n",
    "sample_num_tr = df_train[\"subject\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f0a201-0d7e-4f1b-b53a-78080f9922c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_x2 = df_test['tBodyAcc-mean()-X']\n",
    "acc_y2 = df_test['tBodyAcc-mean()-Y']\n",
    "acc_z2 = df_test['tBodyAcc-mean()-Z']\n",
    "\n",
    "sample_num_te = df_test[\"subject\"]\n",
    "y=df_test[\"Activity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f39b8416-616d-4f30-b7e3-cd4ba4f301bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dummy_dict = {}\n",
    "for i, v in enumerate(y.unique(),1):\n",
    "    dummy_dict[v]=i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3512954f-7f33-4fe1-baf4-cda797864370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'STANDING': 1,\n",
       " 'SITTING': 2,\n",
       " 'LAYING': 3,\n",
       " 'WALKING': 4,\n",
       " 'WALKING_DOWNSTAIRS': 5,\n",
       " 'WALKING_UPSTAIRS': 6}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9ae13f7-4c16-478b-a972-29b37d12fb91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lst = []\n",
    "for v in y:\n",
    "    lst.append([np.array(dummy_dict[v])])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acedd080-9412-46f9-a0fe-cd1ddb14765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 人がいつ切り替わるかを記録する\n",
    "ind = 0\n",
    "sepa_lst_tr = []\n",
    "old_num=1\n",
    "while True:\n",
    "    if len(sample_num_tr)== ind:\n",
    "        sepa_lst_tr.append(ind)\n",
    "        break\n",
    "    if not old_num  == sample_num_tr[ind]:\n",
    "        old_num=sample_num_tr[ind]\n",
    "        sepa_lst_tr.append(ind)\n",
    "        ind+=1\n",
    "        continue\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8662777d-fe62-4ded-9b84-efd2bd6c5a63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 各人のラベルごとにわける\n",
    "\n",
    "ll_tr=[]\n",
    "old_sepa =0\n",
    "lst_tr = []\n",
    "memo =[]\n",
    "for sepa in sepa_lst_tr:\n",
    "    memo =[]\n",
    "    sepa_df=df_train.loc[old_sepa:sepa]\n",
    "    old_ac = sepa_df[\"Activity\"].loc[old_sepa]\n",
    "    sub = sepa_df[\"subject\"].loc[old_sepa]\n",
    "    ind = old_sepa\n",
    "    old_sepa=sepa\n",
    "    c=0\n",
    "    j=0\n",
    "    old_ind =old_sepa\n",
    "    while True:\n",
    "        if c ==len(sepa_df)or( sepa_df[\"subject\"].loc[ind]!=sub):\n",
    "            break\n",
    "        \n",
    "        if not old_ac ==  sepa_df[\"Activity\"].loc[ind]:\n",
    "            old_ac=  sepa_df[\"Activity\"].loc[ind]\n",
    "            memo.append(ind)\n",
    "    \n",
    "            old_ind = ind\n",
    "            # print(sepa_df[\"Activity\"].loc[ind])\n",
    "            j+=1\n",
    "            \n",
    "            continue\n",
    "        ind+=1\n",
    "        c+=1\n",
    "        # j+=1\n",
    "    lst_tr.append(memo)\n",
    "    old_sepa+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad9df7f3-4d76-4d2e-a0e0-68559a533f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 人がいつ切り替わるかを記録する\n",
    "ind = 0\n",
    "sepa_lst_te = []\n",
    "old_num=2\n",
    "while True:\n",
    "    if len(sample_num_te)== ind:\n",
    "        sepa_lst_te.append(ind)\n",
    "        break\n",
    "    if not old_num  == sample_num_te[ind]:\n",
    "        old_num=sample_num_te[ind]\n",
    "        sepa_lst_te.append(ind)\n",
    "        ind+=1\n",
    "        continue\n",
    "    ind+=1\n",
    "\n",
    "\n",
    "## 各人のラベルごとにわける\n",
    "\n",
    "old_sepa =0\n",
    "lst_te = []\n",
    "memo =[]\n",
    "for sepa in sepa_lst_te:\n",
    "    memo =[]\n",
    "    sepa_df=df_test.loc[old_sepa:sepa]\n",
    "    old_ac = sepa_df[\"Activity\"].loc[old_sepa]\n",
    "    sub = sepa_df[\"subject\"].loc[old_sepa]\n",
    "    ind = old_sepa\n",
    "    old_sepa=sepa\n",
    "    c=0\n",
    "    j=0\n",
    "    old_ind =old_sepa\n",
    "    while True:\n",
    "        if c ==len(sepa_df)or( sepa_df[\"subject\"].loc[ind]!=sub):\n",
    "            break\n",
    "        \n",
    "        if not old_ac ==  sepa_df[\"Activity\"].loc[ind]:\n",
    "            old_ac=  sepa_df[\"Activity\"].loc[ind]\n",
    "            memo.append(ind)\n",
    "            old_ind = ind\n",
    "            # print(sepa_df[\"Activity\"].loc[ind])\n",
    "            j+=1\n",
    "            \n",
    "            continue\n",
    "        ind+=1\n",
    "        c+=1\n",
    "        # j+=1\n",
    "    lst_te.append(memo)\n",
    "    old_sepa+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0899f177-b736-4262-83a4-a815ae460a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sp_lst=[]\n",
    "y_v_train =[]\n",
    "x_v_train=[]\n",
    "ol=0\n",
    "for v in lst_tr:\n",
    "    sp_lst=[]\n",
    "    for v1 in v:\n",
    "        d =df_train.loc[ol:v1]\n",
    "        x = d['tBodyAcc-mean()-X']\n",
    "        y = d['tBodyAcc-mean()-Y']\n",
    "        z = d['tBodyAcc-mean()-Z']\n",
    "        fft_x = np.fft.fft(x)\n",
    "        fft_y = np.fft.fft(y)\n",
    "        fft_z = np.fft.fft(z)\n",
    "        v2 = d[\"Activity\"].loc[ol]\n",
    "        y_v_train.append(np.array(dummy_dict[v2]))\n",
    "        ol=v1\n",
    "        x_v_train.append((np.array([fft_x,fft_y,fft_z])))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02421fd1-6b6f-43c0-98e7-c5781a55a68d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_v_test =[]\n",
    "x_v_test =[]\n",
    "ol=0\n",
    "for v in lst_te:\n",
    "    sp_lst=[]\n",
    "    for v1 in v:\n",
    "        d =df_test.loc[ol:v1]\n",
    "        x = d['tBodyAcc-mean()-X']\n",
    "        y = d['tBodyAcc-mean()-Y']\n",
    "        z = d['tBodyAcc-mean()-Z']\n",
    "        v2 = d[\"Activity\"].loc[ol]\n",
    "        y_v_test.append(np.array(dummy_dict[v2]))\n",
    "        ol=v1\n",
    "        x_v_test.append((np.array([fft_x,fft_y,fft_z])))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80acd7c5-e302-440a-9205-175d64e425ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs = 50  # サンプリング周波数 (Hz)\n",
    "\n",
    "def apply_fft(signal, fs):\n",
    "    freq = np.fft.fftfreq(len(signal), d=1/fs)\n",
    "    fft_result = np.fft.fft(signal)\n",
    "    return freq, fft_result\n",
    "\n",
    "def filter_freq(freq, fft_result, min_, max_):\n",
    "    pass_ = (freq >= min_) & (freq <= max_)\n",
    "    filtered_fft = fft_result * pass_\n",
    "    filtered = np.fft.ifft(filtered_fft)\n",
    "    return filtered.real\n",
    "\n",
    "def smooth_padding(signal, pad_length):\n",
    "    \"\"\"\n",
    "    スムーズにゼロに遷移するパディングを追加する。\n",
    "    \"\"\"\n",
    "    taper = np.linspace(signal[-1], 0, pad_length)  # スムーズにゼロに減少\n",
    "    return np.concatenate([signal, taper])\n",
    "\n",
    "def pad_to_length(data, target_length):\n",
    "    current_length = len(data)\n",
    "    if current_length < target_length:\n",
    "        pad_length = target_length - current_length\n",
    "        return smooth_padding(data, pad_length)\n",
    "    return data[:target_length]\n",
    "\n",
    "def process_signal(acc, fs, target_length):\n",
    "    \n",
    "    freq, fft = apply_fft(acc, fs)\n",
    "    low = filter_freq(freq, fft, 0 ,5)\n",
    "    middle = filter_freq(freq, fft,5, 10)\n",
    "    mid = filter_freq(freq, fft, 10, 15)\n",
    "    midd = filter_freq(freq, fft, 15, 20)\n",
    "    high = filter_freq(freq, fft, 20, 25)\n",
    "    # print(freq)\n",
    "    return (\n",
    "        pad_to_length(low, target_length),\n",
    "        pad_to_length(middle, target_length),\n",
    "        pad_to_length(mid, target_length),\n",
    "        pad_to_length(midd, target_length),\n",
    "        pad_to_length(high, target_length),\n",
    "    )\n",
    "\n",
    "x_v11 = []\n",
    "x_v12 = []\n",
    "x_v13 = []\n",
    "x_v14=[]\n",
    "x_v15=[]\n",
    "\n",
    "for v in x_v_train:\n",
    "    acc_x, acc_y, acc_z = v\n",
    "    # 全体のターゲット長を決定\n",
    "    target_length = max(len(acc_x), len(acc_y), len(acc_z))\n",
    "\n",
    "    low_x, middle_x,mid_x,midd_x, high_x = process_signal(acc_x, fs, target_length)\n",
    "    low_y, middle_y,mid_y,midd_y, high_y = process_signal(acc_y, fs, target_length)\n",
    "    low_z, middle_z,mid_z,midd_z, high_z = process_signal(acc_z, fs, target_length)\n",
    "\n",
    "    x_v11.append(np.array([low_x, low_y, low_z]))\n",
    "    x_v13.append(np.array([middle_x, middle_y, middle_z]))\n",
    "    x_v14.append(np.array([mid_x, mid_y, mid_z]))\n",
    "    x_v15.append(np.array([midd_x, midd_y, midd_z]))\n",
    "    x_v12.append(np.array([high_x, high_y, high_z]))\n",
    "\n",
    "\n",
    "# わけたやつを基にもどすところ上からlow,middle,mid,midd,high\n",
    "\n",
    "n = []\n",
    "for i in range(len(x_v_train)):\n",
    "    m=[]\n",
    "    m2=[]\n",
    "    m3=[]\n",
    "    m4=[]\n",
    "    m5=[]\n",
    "    for k in range(len(x_v11[i][0])):\n",
    "        m.append([x_v11[i][0][k],x_v11[i][1][k],x_v11[i][2][k]])\n",
    "        \n",
    "        \n",
    "    for k in range(len(x_v13[i][0])):\n",
    "        m3.append([x_v13[i][0][k],x_v13[i][1][k],x_v13[i][2][k]],)\n",
    "        \n",
    "    \n",
    "    for k in range(len(x_v14[i][0])):\n",
    "        m4.append([x_v14[i][0][k],x_v14[i][1][k],x_v14[i][2][k]],)\n",
    "    \n",
    "    \n",
    "    for k in range(len(x_v15[i][0])):\n",
    "        m5.append([x_v15[i][0][k],x_v15[i][1][k],x_v15[i][2][k]],)\n",
    "    \n",
    "        \n",
    "    for k in range(len(x_v12[i][0])):\n",
    "        m2.append([x_v12[i][0][k],x_v12[i][1][k],x_v12[i][2][k]],)\n",
    "    n.append([np.array(m),np.array(m2)])\n",
    "\n",
    "num_classes=6\n",
    "\n",
    "X_train=n\n",
    "Y_train = np.array(y_v_train)-1\n",
    "Y_train_one_hot = np.eye(num_classes)[Y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27ee3f05-0754-4a41-86c5-a104d0bc8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_v21 = []\n",
    "x_v22 = []\n",
    "x_v23 = []\n",
    "x_v24=[]\n",
    "x_v25=[]\n",
    "\n",
    "for v in x_v_test:\n",
    "    acc_x, acc_y, acc_z = v\n",
    "    # 全体のターゲット長を決定\n",
    "    target_length = max(len(acc_x), len(acc_y), len(acc_z))\n",
    "\n",
    "    low_x, middle_x,mid_x,midd_x, high_x = process_signal(acc_x, fs, target_length)\n",
    "    low_y, middle_y,mid_y,midd_y, high_y = process_signal(acc_y, fs, target_length)\n",
    "    low_z, middle_z,mid_z,midd_z, high_z = process_signal(acc_z, fs, target_length)\n",
    "\n",
    "    x_v21.append(np.array([low_x, low_y, low_z]))\n",
    "    x_v23.append(np.array([middle_x, middle_y, middle_z]))\n",
    "    x_v24.append(np.array([mid_x, mid_y, mid_z]))\n",
    "    x_v25.append(np.array([midd_x, midd_y, midd_z]))\n",
    "    x_v22.append(np.array([high_x, high_y, high_z]))\n",
    "\n",
    "n1 = []\n",
    "for i in range(len(x_v_test)):\n",
    "    m=[]\n",
    "    m2=[]\n",
    "    m3=[]\n",
    "    m4=[]\n",
    "    m5=[]\n",
    "    for k in range(len(x_v21[i][0])):\n",
    "        m.append([x_v21[i][0][k],x_v21[i][1][k],x_v21[i][2][k]])\n",
    "        \n",
    "        \n",
    "    for k in range(len(x_v23[i][0])):\n",
    "        m3.append([x_v23[i][0][k],x_v23[i][1][k],x_v23[i][2][k]],)\n",
    "        \n",
    "    \n",
    "    for k in range(len(x_v24[i][0])):\n",
    "        m4.append([x_v24[i][0][k],x_v24[i][1][k],x_v24[i][2][k]],)\n",
    "    \n",
    "    \n",
    "    for k in range(len(x_v25[i][0])):\n",
    "        m5.append([x_v25[i][0][k],x_v25[i][1][k],x_v25[i][2][k]],)\n",
    "    \n",
    "        \n",
    "    for k in range(len(x_v22[i][0])):\n",
    "        m2.append([x_v22[i][0][k],x_v22[i][1][k],x_v22[i][2][k]],)\n",
    "    n1.append([np.array(m),np.array(m2)])\n",
    "    \n",
    "X_test=n1\n",
    "Y_test = np.array(y_v_test)-1\n",
    "Y_test_one_hot = np.eye(num_classes)[Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17669a8f-c755-4c87-a9d1-1fbde99a92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ParallelDeepESN:\n",
    "    def __init__(self, input_size, reservoir_sizes, num_classes, sparsity=0.1, spectral_radius=0.95):\n",
    "        \"\"\"\n",
    "        Parallel Deep ESNモデルの初期化\n",
    "        :param input_size: 各周波数帯データの入力次元\n",
    "        :param reservoir_sizes: 各リザバーのサイズ（各周波数帯ごと: [size1, size2, ..., sizeM]）\n",
    "        :param num_classes: 出力のクラス数\n",
    "        :param sparsity: リザバーのスパース性（結合の密度）\n",
    "        :param spectral_radius: スペクトル半径（リザバーの安定性を制御）\n",
    "        \"\"\"\n",
    "        self.num_processes = len(reservoir_sizes)  # 周波数帯の数（リザバー層数）\n",
    "        self.reservoirs = []\n",
    "        self.weights_input = []\n",
    "        self.weights_reservoir = []\n",
    "\n",
    "        # 各リザバー層の初期化\n",
    "        for reservoir_size in reservoir_sizes:\n",
    "            # 入力 -> リザバーの重み行列\n",
    "            W_in = (np.random.rand(reservoir_size, input_size) - 0.5) * 1.0\n",
    "            # リザバーの重み行列\n",
    "            W_res = (np.random.rand(reservoir_size, reservoir_size) - 0.5) * 1.0\n",
    "            W_res[np.random.rand(*W_res.shape) > sparsity] = 0  # スパース化\n",
    "            eigvals = np.max(np.abs(np.real(np.linalg.eigvals(W_res))))  # 実数成分のみ使用\n",
    "            W_res *= spectral_radius / eigvals  # スペクトル半径調整\n",
    "\n",
    "            self.weights_input.append(W_in)\n",
    "            self.weights_reservoir.append(W_res)\n",
    "            self.reservoirs.append(np.zeros((reservoir_size, 1)))  # 初期状態（ゼロ）\n",
    "\n",
    "        # 出力層の重み行列（学習対象）\n",
    "        self.W_output = np.zeros((num_classes, sum(reservoir_sizes)))  # 総リザバー出力サイズ\n",
    "\n",
    "    def forward(self, inputs_list):\n",
    "        \"\"\"\n",
    "        各周波数帯ごとに独立したリザバーを更新\n",
    "        :param inputs_list: 1つのサンプルにおける周波数帯データのリスト（[周波数帯1, 周波数帯2, ...]）\n",
    "        :return: 統合されたリザバー状態ベクトル\n",
    "        \"\"\"\n",
    "        all_reservoir_states = []\n",
    "        for i, inputs in enumerate(inputs_list):\n",
    "            W_in = self.weights_input[i]\n",
    "            W_res = self.weights_reservoir[i]\n",
    "            reservoir = self.reservoirs[i]\n",
    "\n",
    "            reservoir_states = []  # 時系列ごとのリザバー状態を保存\n",
    "            for t in range(inputs.shape[0]):  # 時系列を順に処理\n",
    "                u_t = inputs[t].reshape(-1, 1)  # 列ベクトル化\n",
    "                reservoir = np.tanh(np.dot(W_res, reservoir) + np.dot(W_in, u_t))\n",
    "                reservoir = np.real(reservoir)  # 複素数成分を防ぐため実数部分のみを保持\n",
    "                reservoir_states.append(reservoir)\n",
    "\n",
    "            # 時系列データを時間方向に平均化（または他の統計量を使用可能）\n",
    "            aggregated_reservoir = np.mean(np.hstack(reservoir_states), axis=1, keepdims=True)\n",
    "            \n",
    "            \n",
    "            reservoir_states_stacked = np.hstack(reservoir_states)\n",
    "            mean_reservoir = np.mean(reservoir_states_stacked, axis=1, keepdims=True)\n",
    "            var_reservoir = np.var(reservoir_states_stacked, axis=1, keepdims=True)\n",
    "            aggregated_reservoir = np.vstack([mean_reservoir, var_reservoir])\n",
    "\n",
    "            \n",
    "            all_reservoir_states.append(aggregated_reservoir)\n",
    "\n",
    "        # 各周波数帯のリザバー状態を連結して返す\n",
    "        return np.vstack(all_reservoir_states)\n",
    "\n",
    "    def train(self, X, targets_one_hot, regularization=1e-12):\n",
    "        \"\"\"\n",
    "        出力層の重み W_output を学習（リッジ回帰を使用）\n",
    "        :param X: 入力データ（各サンプルの [[周波数帯1, 周波数帯2, ...], ...]）\n",
    "        :param targets_one_hot: one-hotエンコードされたターゲット（例: クラス数 x サンプル数）\n",
    "        :param regularization: 正則化項\n",
    "        \"\"\"\n",
    "        reservoir_states = []\n",
    "        for inputs_list in X:\n",
    "            reservoir_states.append(self.forward(inputs_list))\n",
    "\n",
    "        # 状態行列 X を構築\n",
    "        X_concat = np.hstack(reservoir_states)  # 各サンプルの最終リザバー状態を列として並べる\n",
    "        Y = np.array(targets_one_hot).T  # ワンホットターゲットを転置して (num_classes, num_samples)\n",
    "\n",
    "        # エラー回避のための形状チェック\n",
    "        if X_concat.shape[1] < X_concat.shape[0]:\n",
    "            print(\"警告: サンプル数がリザバー状態の次元より少ないため、逆行列計算が不安定になる可能性があります。\")\n",
    "            # 正則化を増やして安定性を確保\n",
    "            regularization = max(regularization, 1e-2)\n",
    "        # regularization=1e-8\n",
    "        # regularization = 1e-4 * np.linalg.norm(np.dot(X_concat, X_concat.T))\n",
    "        # X_concat = (X_concat - np.min(X_concat)) / (np.max(X_concat) - np.min(X_concat))\n",
    "        # X_concat = (X_concat - np.min(X_concat)) / (np.max(X_concat) - np.min(X_concat))\n",
    "        # X_concat = (X_concat - np.mean(X_concat, axis=1, keepdims=True)) / np.std(X_concat, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "        # リッジ回帰で W_output を計算\n",
    "        self.W_output = np.dot(\n",
    "            np.dot(Y, X_concat.T),\n",
    "            np.linalg.inv(np.dot(X_concat, X_concat.T) + regularization * np.eye(X_concat.shape[0]))\n",
    "        )\n",
    "\n",
    "        # 訓練データでのクロスエントロピーを計算\n",
    "   \n",
    "\n",
    "        logits = np.real(np.dot(self.W_output, X_concat))  # 実数成分のみを保持\n",
    "        softmax_probs = np.exp(logits) / np.sum(np.exp(logits), axis=0)\n",
    "        cross_entropy_loss = -np.sum(Y * np.log(softmax_probs + 1e-12)) / Y.shape[1]\n",
    "        print(f\"Training Cross-Entropy Loss: {cross_entropy_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X, targets_one_hot=None):\n",
    "        \"\"\"\n",
    "        複数の入力データ群を用いてクラスを予測し、必要に応じてクロスエントロピー損失を計算\n",
    "        :param X: 入力データ（各サンプルの [[周波数帯1, 周波数帯2, ...], ...] リスト）\n",
    "        :param targets_one_hot: one-hotエンコードされたターゲット（例: クラス数 x サンプル数）。指定された場合にクロスエントロピーを計算。\n",
    "        :return: 各サンプルの予測クラス、必要に応じてクロスエントロピー損失\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        reservoir_states = []\n",
    "\n",
    "        for inputs_list in X:\n",
    "            final_state = self.forward(inputs_list)\n",
    "            logits = np.real(np.dot(self.W_output, final_state))  # 実数成分のみを保持\n",
    "            predicted_class = np.argmax(logits, axis=0)  # クラスを予測\n",
    "            predictions.append(predicted_class)\n",
    "            reservoir_states.append(final_state)\n",
    "\n",
    "        # 必要に応じてクロスエントロピーを計算\n",
    "        cross_entropy_loss = None\n",
    "        if targets_one_hot is not None:\n",
    "            # 状態行列を再構築\n",
    "            X_concat = np.hstack(reservoir_states)  # 各サンプルのリザバー状態を結合\n",
    "            Y = np.array(targets_one_hot).T  # ワンホットターゲットを転置 (num_classes, num_samples)\n",
    "\n",
    "            # ロジット（予測値の前処理）\n",
    "            logits = np.real(np.dot(self.W_output, X_concat))  # 実数成分のみを保持\n",
    "            softmax_probs = np.exp(logits) / np.sum(np.exp(logits), axis=0)  # ソフトマックス\n",
    "            cross_entropy_loss = -np.sum(Y * np.log(softmax_probs + 1e-12)) / Y.shape[1]  # クロスエントロピー計算\n",
    "            print(f\"Prediction Cross-Entropy Loss: {cross_entropy_loss:.4f}\")\n",
    "\n",
    "        return predictions, cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d9586c1-09e1-4907-ba44-80e7a41a1e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Entropy Loss: 1.2988\n",
      "Prediction Cross-Entropy Loss: 1.7945\n"
     ]
    }
   ],
   "source": [
    "# サンプル数と設定\n",
    "reservoir_sizes = [25,25,25,25,25]  # 工程数をnに増加\n",
    "num_classes = 6\n",
    "input_size = 3\n",
    "\n",
    "# モデルの初期化と学習\n",
    "model = ParallelDeepESN(input_size=input_size, reservoir_sizes=reservoir_sizes, num_classes=num_classes,\n",
    "                       sparsity=0.8, spectral_radius=0.95)\n",
    "model.train(X_train, Y_train_one_hot)\n",
    "\n",
    "predictions, loss = model.predict(X_test, targets_one_hot=Y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac0d236f-b75c-4a4b-aa91-2cc128c5bed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "混同行列:\n",
      " [[ 0  0  0  0 10  0]\n",
      " [ 0  0  0  0 18  0]\n",
      " [ 0  0  0  0 18  0]\n",
      " [ 0  0  0  0 18  0]\n",
      " [ 0  0  0  0 25  0]\n",
      " [ 0  0  0  0 22  0]]\n",
      "F1スコア (マクロ平均): 0.06127450980392157\n",
      "\n",
      "分類レポート:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "          Standing       0.00      0.00      0.00        10\n",
      "           Sitting       0.00      0.00      0.00        18\n",
      "            Laying       0.00      0.00      0.00        18\n",
      "           Walking       0.00      0.00      0.00        18\n",
      "Walking_downstairs       0.23      1.00      0.37        25\n",
      "   Waling_Upstairs       0.00      0.00      0.00        22\n",
      "\n",
      "          accuracy                           0.23       111\n",
      "         macro avg       0.04      0.17      0.06       111\n",
      "      weighted avg       0.05      0.23      0.08       111\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "t =[]\n",
    "for i in predictions:\n",
    "    t.append(i[0])\n",
    "t=np.array(t)\n",
    "true_labels = Y_test\n",
    "predicted_labels = t\n",
    "\n",
    "# 混同行列の計算\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "print(\"混同行列:\\n\", conf_matrix)\n",
    "\n",
    "# F1スコア（マクロ平均）\n",
    "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "print(\"F1スコア (マクロ平均):\", f1)\n",
    "\n",
    "# 詳細なレポート\n",
    "\n",
    "report = classification_report(true_labels, predicted_labels, target_names=[\"Standing\",\n",
    "\"Sitting\",\"Laying\",\"Walking\",\"Walking_downstairs\", \"Waling_Upstairs\"])\n",
    "print(\"\\n分類レポート:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea611d3-5ed0-4a5b-be42-83b00fc5d374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c369ff-af86-4bb6-ad7e-d1420f9eda06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
